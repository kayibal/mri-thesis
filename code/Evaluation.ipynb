{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import sqlite3\n",
    "import random\n",
    "import tables\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "from collections import Counter\n",
    "import graphlab as gl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.html.widgets import FloatProgress\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"/home/kayibal/sc-recom/code/msd_data/final_features.npy\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/kayibal/sc-recom/code/msd_data/eval_dic.p\", \"r\") as f:\n",
    "    splitted_user_likes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/kayibal/sc-recom/code/msd_data/eval_dic_big.p\", \"r\") as f:\n",
    "    splitted_likes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/kayibal/sc-recom/code/msd_data/like_dic.p\", \"r\") as f:\n",
    "    user_likes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/kayibal/sc-recom/code/msd_data/like_dic_big.p\", \"r\") as f:\n",
    "    user_likes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load feature vectos\n",
    "f = tables.open_file(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/analysis2.h5\", mode=\"r\")\n",
    "\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "\n",
    "pca = PCA(n_components=60)\n",
    "flucs_comp = pca.fit_transform(np.asarray(minmax.fit_transform(f.root.analysis.flucs)).transpose()).transpose()\n",
    "\n",
    "data = {'A':{},'B':{},'C':{},'D':{},'E':{},'F':{},'G':{},'H':{},'I':{},'J':{}}\n",
    "data['A']['desc'] = \"Fluctuation Patterns + Gaussian Concatenated MFCC Representations\"\n",
    "data['A']['data'] = np.vstack((np.array(flucs_comp), minmax.fit_transform(np.array(f.root.analysis.gaussian_rep)))).transpose()\n",
    "data['B']['desc'] = \"Fluctuation Patterns + Concatenated MFCC Medians\"\n",
    "data['B']['data'] = np.vstack((np.array(flucs_comp), minmax.fit_transform(np.array(f.root.analysis.mfcc_medians_comp)))).transpose()\n",
    "data['C']['desc'] = \"Fluctuation Patterns + Concatenated MFCC Delta Medians\"\n",
    "data['C']['data'] = np.vstack((np.array(flucs_comp), minmax.fit_transform(np.array(f.root.analysis.mfcc_delta_medians_comp)))).transpose()\n",
    "data['D']['desc'] = \"Fluctuation Patterns + Concatenated MFCC Medians and Delta Medians\"\n",
    "data['D']['data'] = np.vstack((np.array(flucs_comp), minmax.fit_transform(np.array(f.root.analysis.mfcc_medians_comp)), minmax.fit_transform(np.array(f.root.analysis.mfcc_delta_medians_comp)))).transpose()\n",
    "data['E']['desc'] = \"Fluctuation Patterns Compressed\"\n",
    "data['E']['data'] = np.array(flucs_comp).transpose()\n",
    "data['F']['desc'] = \"Concatenated MFCC Medians\"\n",
    "data['F']['data'] = minmax.fit_transform(np.array(f.root.analysis.mfcc_medians_comp).transpose())\n",
    "data['G']['desc'] = \"Concatenated MFCC Delta Medians\"\n",
    "data['G']['data'] = minmax.fit_transform(np.array(f.root.analysis.mfcc_delta_medians_comp).transpose())\n",
    "data['H']['desc'] = \"Concatenated MFCC Medians and Delta Medians\"\n",
    "data['H']['data'] = np.vstack((minmax.fit_transform(np.array(f.root.analysis.mfcc_medians_comp)), minmax.fit_transform(np.array(f.root.analysis.mfcc_delta_medians_comp)))).transpose()\n",
    "data['I']['desc'] = \"Gaussian Concatenated MFCC Representations\"\n",
    "data['I']['data'] = minmax.fit_transform(np.array(f.root.analysis.gaussian_rep)).transpose()\n",
    "data['J']['desc'] = \"Raw Fluctuation Patterns\"\n",
    "data['J']['data'] = np.array(f.root.analysis.flucs).transpose()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "PROGRESS: Recsys training: model = item_similarity\n",
      "PROGRESS: Preparing data set.\n",
      "PROGRESS:     Data has 11925 observations with 250 users and 7366 items.\n",
      "PROGRESS:     Data prepared in: 0.040732s\n",
      "PROGRESS: Computing item similarity statistics:\n",
      "PROGRESS: Computing most similar items for 7366 items:\n",
      "PROGRESS: +-----------------+-----------------+\n",
      "PROGRESS: | Number of items | Elapsed Time    |\n",
      "PROGRESS: +-----------------+-----------------+\n",
      "PROGRESS: | 1000            | 1.60743         |\n",
      "PROGRESS: | 2000            | 1.66643         |\n",
      "PROGRESS: | 3000            | 1.71731         |\n",
      "PROGRESS: | 4000            | 1.75434         |\n",
      "PROGRESS: | 5000            | 1.7868          |\n",
      "PROGRESS: | 6000            | 1.93495         |\n",
      "PROGRESS: | 7000            | 2.07185         |\n",
      "PROGRESS: +-----------------+-----------------+\n",
      "PROGRESS: Finished training in 2.28384s\n"
     ]
    }
   ],
   "source": [
    "#make sframe from user likes\n",
    "likes = {'user_id':[], 'item_id':[]}\n",
    "for user in splitted_likes.keys():\n",
    "    for track in splitted_likes[user]['rec']:\n",
    "        likes['user_id'].append(user)\n",
    "        likes['item_id'].append(track)\n",
    "print len(Counter(likes['user_id']))\n",
    "sf = gl.SFrame(likes)\n",
    "cf = gl.recommender.create(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Recsys training: model = popularity\n",
      "PROGRESS: Preparing data set.\n",
      "PROGRESS:     Data has 651754 observations with 16817 users and 7979 items.\n",
      "PROGRESS:     Data prepared in: 1.04798s\n",
      "PROGRESS: 651754 observations to process; with 7979 unique items.\n"
     ]
    }
   ],
   "source": [
    "popcf = gl.popularity_recommender.create(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#database\n",
    "conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT msd_id FROM tracks WHERE audio_path IS NOT NULL ORDER BY msd_id\")\n",
    "tracks = c.fetchall()\n",
    "conn.close()\n",
    "tracks = np.array([el[0] for el in tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_liked_clusters(cluster,likes):\n",
    "    liked_clusters = []\n",
    "    query = \"SELECT %s_cluster FROM tracks WHERE msd_id = '%s'\"\n",
    "    \n",
    "    conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "    c = conn.cursor()\n",
    "    for msd_id in likes:\n",
    "        c.execute(query % (cluster,msd_id))\n",
    "        liked_clusters.append(c.fetchone()[0])\n",
    "    conn.close()\n",
    "    return liked_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_recommendations(cluster, likes, remove_likes=True):\n",
    "    liked_clusters = set(retrieve_liked_clusters(cluster,likes))\n",
    "    #print \"this user liked %d of 400 clusters\" % len(liked_clusters)\n",
    "    rec = []\n",
    "    query = \"SELECT msd_id FROM tracks WHERE %s_cluster = '%d'\"\n",
    "    conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "    c = conn.cursor()\n",
    "    for c_id in set(liked_clusters):\n",
    "        c.execute(query%(cluster,c_id))\n",
    "        rec += [r[0] for r in c.fetchall()]\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        recs = filter(lambda x: x not in itemsToRemove,recs)\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_recommendations_freq_cluster(cluster, likes, cutoff, remove_likes=True):\n",
    "    liked_clusters = [el[0] for el in Counter(retrieve_liked_clusters(cluster,likes)).most_common()[:cutoff]]\n",
    "    #print \"this user liked %d of 400 clusters\" % len(liked_clusters)\n",
    "    rec = []\n",
    "    query = \"SELECT msd_id FROM tracks WHERE %s_cluster = '%d'\"\n",
    "    conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "    c = conn.cursor()\n",
    "    for c_id in set(liked_clusters):\n",
    "        c.execute(query%(cluster,c_id))\n",
    "        rec += [r[0] for r in c.fetchall()]\n",
    "    conn.close()\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        rec = filter(lambda x: x not in itemsToRemove,rec)\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(msd_ids, data):\n",
    "    idx_arr = []\n",
    "    ids = set(msd_ids)\n",
    "    for i,t in enumerate(tracks):\n",
    "        if t in ids:\n",
    "            idx_arr.append(i)\n",
    "    return data[:,idx_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_recommendations_scored(cluster, likes, cutoff, data, remove_likes=True):\n",
    "    liked_clusters = [el[0] for el in Counter(retrieve_liked_clusters(cluster,likes)).most_common()]\n",
    "    #print \"this user liked %d of 400 clusters\" % len(liked_clusters)\n",
    "    rec = []\n",
    "    query = \"SELECT msd_id FROM tracks WHERE %s_cluster = '%d'\"\n",
    "    conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "    c = conn.cursor()\n",
    "    for c_id in set(liked_clusters):\n",
    "        c.execute(query%(cluster,c_id))\n",
    "        rec += [r[0] for r in c.fetchall()]\n",
    "    conn.close()\n",
    "    rec.sort()\n",
    "    #calculate distance matrix between likes and recs\n",
    "    like_features = get_features(likes,data)\n",
    "    rec_features = get_features(rec,data)\n",
    "    scores = np.sum(cdist(like_features.T,rec_features.T,'euclidean'), axis=0)\n",
    "    scored_recs = sorted(zip(rec,scores), key= lambda x: x[1])\n",
    "    recs = [el[0] for el in scored_recs]\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        recs = filter(lambda x: x not in itemsToRemove,recs)\n",
    "    return recs[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_recommendations_cluster_scored(cluster, likes, cutoff, data, k=3, remove_likes=True, score=\"distance\"):\n",
    "    liked_clusters = [el[0] for el in Counter(retrieve_liked_clusters(cluster,likes)).most_common()][:k]\n",
    "    like_set = set(likes)\n",
    "    recs = []\n",
    "    query = \"SELECT msd_id FROM tracks WHERE %s_cluster = '%d' ORDER BY msd_id\"\n",
    "    conn = sqlite3.connect(\"/home/kayibal/sc-recom/code/msd_data/analysis_data/sampled_likes_tracks.db\")\n",
    "    c = conn.cursor()\n",
    "    for c_id in set(liked_clusters):\n",
    "        c.execute(query%(cluster, c_id))\n",
    "        temp = [x[0] for x in c.fetchall()]\n",
    "        if score == \"distance\":\n",
    "            #likes which belong to c_id\n",
    "            c_likes = set(temp) & like_set\n",
    "            temp = filter(lambda x: x not in like_set,temp)\n",
    "            #calculate score and normalize\n",
    "            like_features = get_features(c_likes,data)\n",
    "            rec_features = get_features(temp,data)\n",
    "            try:\n",
    "                scores = np.min(cdist(like_features.T,rec_features.T,'euclidean'), axis=0)\n",
    "                scores = scores / np.max(scores)\n",
    "                scored_recs = sorted(zip(temp,scores), key= lambda x: x[1])\n",
    "                recs += scored_recs\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            recs += list(popcf.recommend(items=temp)['item_id'])[:(cutoff/k)]\n",
    "    conn.close()\n",
    "    #calculate distance matrix between likes and recs\n",
    "    if score == \"distance\":\n",
    "        recs = sorted(recs, key= lambda x: x[1])\n",
    "        recs = [el[0] for el in recs]\n",
    "    if remove_likes:\n",
    "        recs = filter(lambda x: x not in like_set,recs)\n",
    "    return recs[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_matrix_recommender(likes, feature, cutoff):\n",
    "    recs=[]\n",
    "    dists = []\n",
    "    for like in likes:\n",
    "        like_idx = np.where(tracks == like)[0][0]\n",
    "        feature_vec = data[feature]['data'][like_idx:like_idx+1,:]\n",
    "        distances = cdist(feature_vec, data[feature]['data']).flatten()\n",
    "        indizes = np.argsort(distances)[1:cutoff/len(likes)]\n",
    "        dists += list(distances[1:cutoff/len(likes)])\n",
    "        recs += list(tracks[indizes])\n",
    "    scored_recs = zip(recs,dists)\n",
    "    scored_recs = sorted(scored_recs, key = lambda x:x[1])\n",
    "    return [el[0] for el in scored_recs][:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cf_recommendations(likes, cutoff,remove_likes=True):\n",
    "    recs = cf.get_similar_items(items = likes).sort('score', ascending=False)['similar']\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        recs = filter(lambda x: x not in itemsToRemove,recs)\n",
    "    return recs[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cf_user_recommendations(user, cutoff,remove_likes=True):\n",
    "    recs = cf.recommend([user],k=4000).sort('score', ascending=False)['item_id']\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        recs = filter(lambda x: x not in itemsToRemove,recs)\n",
    "    return recs[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_random_recommendations(length,remove_likes=True):\n",
    "    recs = random.sample(tracks, length+len(likes))\n",
    "    if remove_likes:\n",
    "        itemsToRemove = set(likes)\n",
    "        recs = filter(lambda x: x not in itemsToRemove,recs)\n",
    "    return recs[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    row_html = \"<tr><td>%d</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td></tr>\"\n",
    "    first_row = \"<tr><td rowspan='%d'>%s</td><td>%d</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td><td>%.2f%% (&#177; %.2f)</td></tr>\"\n",
    "    html = \"<table style='width:100%'>\"\n",
    "    #headers\n",
    "    html += \"<tr><th>%s</th><th>%s</th><th>%s</th><th>%s</th><th>%s</th><th>%s</th></tr>\" % (\"Feature</br> Vector\",\"Clusters used\",\"Our Approach\", \"CF Recommender\",\"Random Recommender\", \"Recommendation Size (% of total songs)\")\n",
    "    for key in scores.keys():\n",
    "        first = True\n",
    "        for row in scores[key]:\n",
    "            if first:\n",
    "                html += first_row % (len(scores[key]), key, row['cutoff'], row['tp_mean'], row['tp_std'], row['tp_cf_mean'], row['tp_cf_std'], row['tp_rand_mean'], row['tp_rand_std'], row['per_mean'], row['per_std'])\n",
    "                first = False\n",
    "            else:\n",
    "                html += row_html % (row['cutoff'], row['tp_mean'], row['tp_std'], row['tp_cf_mean'], row['tp_cf_std'], row['tp_rand_mean'], row['tp_rand_std'], row['per_mean'], row['per_std'])\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores_latex(scores):\n",
    "    \n",
    "    row_string = \"& $%d$ & $%.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $ \\\\\\\\ \\\\cline{2-6} \\n\"\n",
    "    first_row = \"\\\\hline \\\\hline \\\\multirow{%d}{*}{%s} & $%d$ &$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f)$ \\\\\\\\ \\\\cline{2-6} \\n\"\n",
    "    html = \"\\\\begin{tabular}{| l || c | c | c | c | c |}\\n \\\\hline \\n\"\n",
    "    #headers\n",
    "    html += \"\\\\rowcolor{lightgray} & %s & %s & %s & %s & %s \\\\\\\\ \\\\hline \\n\" % (\"Clusters used\",\"Our Approach\", \"CF Recommender\",\"Random Recommender\", \"Recommendation Size (\\\\% of total songs)\")\n",
    "    for key in scores.keys():\n",
    "        first = True\n",
    "        for row in scores[key]:\n",
    "            if first:\n",
    "                html += first_row % (len(scores[key]), key, row['cutoff'], row['tp_mean'], row['tp_std'], row['tp_cf_mean'], row['tp_cf_std'], row['tp_rand_mean'], row['tp_rand_std'], row['per_mean'], row['per_std'])\n",
    "                first = False\n",
    "            else:\n",
    "                html += row_string % (row['cutoff'], row['tp_mean'], row['tp_std'], row['tp_cf_mean'], row['tp_cf_std'], row['tp_rand_mean'], row['tp_rand_std'], row['per_mean'], row['per_std'])\n",
    "    html += \"\\\\end{tabular}\\n\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{| l || c | c | c | c | c |}\n",
      " \\hline \n",
      "\\rowcolor{lightgray} & Clusters used & Our Approach & CF Recommender & Random Recommender & Recommendation Size (\\% of total songs) \\\\ \\hline \n",
      "\\hline \\hline \\multirow{4}{*}{A} & $1$ &$ 0.87\\% (\\pm 4.37) $&$ 0.84\\% (\\pm 4.34) $&$ 0.47\\% (\\pm 3.62) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.86\\% (\\pm 9.36) $&$ 3.21\\% (\\pm 8.88) $&$ 3.28\\% (\\pm 8.93) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $13.89\\% (\\pm 17.77) $&$ 9.73\\% (\\pm 13.62) $&$ 12.02\\% (\\pm 13.96) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $27.49\\% (\\pm 21.36) $&$ 18.26\\% (\\pm 19.34) $&$ 25.40\\% (\\pm 20.99) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{C} & $1$ &$ 0.62\\% (\\pm 4.19) $&$ 0.84\\% (\\pm 4.34) $&$ 0.58\\% (\\pm 3.40) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.72\\% (\\pm 9.05) $&$ 3.21\\% (\\pm 8.88) $&$ 2.90\\% (\\pm 7.79) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $13.12\\% (\\pm 16.16) $&$ 9.73\\% (\\pm 13.62) $&$ 12.94\\% (\\pm 16.34) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $27.27\\% (\\pm 21.22) $&$ 18.26\\% (\\pm 19.34) $&$ 27.75\\% (\\pm 22.25) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{B} & $1$ &$ 0.78\\% (\\pm 4.23) $&$ 0.84\\% (\\pm 4.34) $&$ 0.28\\% (\\pm 1.84) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.68\\% (\\pm 8.58) $&$ 3.21\\% (\\pm 8.88) $&$ 3.58\\% (\\pm 8.83) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $14.49\\% (\\pm 18.15) $&$ 9.73\\% (\\pm 13.62) $&$ 13.19\\% (\\pm 15.58) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $26.62\\% (\\pm 21.08) $&$ 18.26\\% (\\pm 19.34) $&$ 26.82\\% (\\pm 21.99) $&$ 24.98\\% (\\pm 0.15) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{E} & $1$ &$ 1.12\\% (\\pm 4.89) $&$ 0.84\\% (\\pm 4.34) $&$ 0.60\\% (\\pm 3.11) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.80\\% (\\pm 9.17) $&$ 3.21\\% (\\pm 8.88) $&$ 2.93\\% (\\pm 9.00) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $14.36\\% (\\pm 16.25) $&$ 9.73\\% (\\pm 13.62) $&$ 12.02\\% (\\pm 14.59) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $27.45\\% (\\pm 21.77) $&$ 18.26\\% (\\pm 19.34) $&$ 26.85\\% (\\pm 22.45) $&$ 24.99\\% (\\pm 0.04) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{D} & $1$ &$ 1.20\\% (\\pm 5.82) $&$ 0.84\\% (\\pm 4.34) $&$ 0.58\\% (\\pm 4.12) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $4.16\\% (\\pm 11.04) $&$ 3.21\\% (\\pm 8.88) $&$ 3.06\\% (\\pm 7.99) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $17.35\\% (\\pm 19.28) $&$ 9.73\\% (\\pm 13.62) $&$ 11.38\\% (\\pm 15.10) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $30.58\\% (\\pm 23.28) $&$ 18.26\\% (\\pm 19.34) $&$ 24.78\\% (\\pm 21.42) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{G} & $1$ &$ 0.84\\% (\\pm 4.24) $&$ 0.84\\% (\\pm 4.34) $&$ 0.73\\% (\\pm 4.27) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.75\\% (\\pm 9.09) $&$ 3.21\\% (\\pm 8.88) $&$ 3.22\\% (\\pm 8.95) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $11.64\\% (\\pm 15.65) $&$ 9.73\\% (\\pm 13.62) $&$ 13.27\\% (\\pm 18.05) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $25.29\\% (\\pm 21.48) $&$ 18.26\\% (\\pm 19.34) $&$ 24.63\\% (\\pm 21.09) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{F} & $1$ &$ 0.74\\% (\\pm 4.17) $&$ 0.84\\% (\\pm 4.34) $&$ 0.62\\% (\\pm 3.53) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.53\\% (\\pm 8.82) $&$ 3.21\\% (\\pm 8.88) $&$ 3.25\\% (\\pm 9.89) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $13.70\\% (\\pm 16.81) $&$ 9.73\\% (\\pm 13.62) $&$ 13.17\\% (\\pm 17.06) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $25.46\\% (\\pm 21.67) $&$ 18.26\\% (\\pm 19.34) $&$ 25.73\\% (\\pm 21.97) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{I} & $1$ &$ 0.61\\% (\\pm 3.58) $&$ 0.84\\% (\\pm 4.34) $&$ 0.62\\% (\\pm 3.56) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.08\\% (\\pm 8.79) $&$ 3.21\\% (\\pm 8.88) $&$ 3.37\\% (\\pm 9.25) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $13.05\\% (\\pm 17.11) $&$ 9.73\\% (\\pm 13.62) $&$ 12.71\\% (\\pm 16.47) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $25.07\\% (\\pm 22.29) $&$ 18.26\\% (\\pm 19.34) $&$ 25.81\\% (\\pm 21.06) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{H} & $1$ &$ 0.86\\% (\\pm 4.73) $&$ 0.84\\% (\\pm 4.34) $&$ 1.52\\% (\\pm 6.75) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.61\\% (\\pm 8.82) $&$ 3.21\\% (\\pm 8.88) $&$ 2.89\\% (\\pm 8.17) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $15.23\\% (\\pm 18.47) $&$ 9.73\\% (\\pm 13.62) $&$ 11.89\\% (\\pm 15.13) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $29.74\\% (\\pm 22.68) $&$ 18.26\\% (\\pm 19.34) $&$ 24.77\\% (\\pm 21.52) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\hline \\hline \\multirow{4}{*}{J} & $1$ &$ 0.71\\% (\\pm 4.59) $&$ 0.84\\% (\\pm 4.34) $&$ 0.57\\% (\\pm 4.21) $&$ 0.62\\% (\\pm 0.00)$ \\\\ \\cline{2-6} \n",
      "& $5$ & $3.82\\% (\\pm 9.48) $&$ 3.21\\% (\\pm 8.88) $&$ 2.92\\% (\\pm 8.41) $&$ 3.12\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $20$ & $13.38\\% (\\pm 16.97) $&$ 9.73\\% (\\pm 13.62) $&$ 12.39\\% (\\pm 15.03) $&$ 12.50\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "& $40$ & $26.26\\% (\\pm 23.12) $&$ 18.26\\% (\\pm 19.34) $&$ 25.67\\% (\\pm 21.79) $&$ 24.99\\% (\\pm 0.00) $ \\\\ \\cline{2-6} \n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print scores_latex(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(cluster, like_dict, cutoffs):\n",
    "    f = FloatProgress(min=0, max=100)\n",
    "    display(f)\n",
    "    scores = []\n",
    "    for c in cutoffs:\n",
    "        user_scores = []\n",
    "        idx = 0\n",
    "        for user in like_dict.keys():\n",
    "            f.value = idx*100./len(like_dict.keys())\n",
    "            f.description = \"current cutoff: %d\" % c\n",
    "            idx += 1\n",
    "            #recs = make_recommendations_freq_cluster(cluster, like_dict[user]['rec'],c)\n",
    "            recs = make_recommendations_cluster_scored(cluster, like_dict[user]['rec'],c*50,data[cluster]['data'].T,k=c)\n",
    "            #recs = dist_matrix_recommender(like_dict[user]['rec'], cluster, c)\n",
    "            rand_recs = make_random_recommendations(len(recs))\n",
    "            cf_recs = make_cf_user_recommendations(user, len(recs))\n",
    "            tp = 0\n",
    "            tp_cf = 0\n",
    "            tp_r = 0\n",
    "            percentage = len(recs)*100./8003.\n",
    "            for track in like_dict[user]['eval']:\n",
    "                if track in recs:\n",
    "                    tp += 1\n",
    "                if track in rand_recs:\n",
    "                    tp_r += 1\n",
    "                if track in cf_recs:\n",
    "                    tp_cf += 1\n",
    "            score = tp*100./len(like_dict[user]['eval'])\n",
    "            score_cf = tp_cf*100./len(like_dict[user]['eval'])            \n",
    "            score_random = tp_r*100./len(like_dict[user]['eval'])\n",
    "            user_scores.append([score, score_cf, score_random, percentage])\n",
    "        mean = np.mean(user_scores, axis=0)\n",
    "        std = np.std(user_scores, axis=0)\n",
    "        scores.append({'cutoff':c,'tp_mean':mean[0], 'tp_std':std[0], 'tp_cf_mean':mean[1], 'tp_cf_std':std[1], 'tp_rand_mean':mean[2], 'tp_rand_std':std[2], 'per_mean':mean[3], 'per_std':std[3]})\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-336-b867a116520e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'ABCDEFGHIJ'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mall_scores_mod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitted_likes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-275-c80be4f7e1fb>\u001b[0m in \u001b[0;36mget_scores\u001b[1;34m(cluster, like_dict, cutoffs)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m#recs = make_recommendations_freq_cluster(cluster, like_dict[user]['rec'],c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mrecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_recommendations_cluster_scored\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;31m#recs = dist_matrix_recommender(like_dict[user]['rec'], cluster, c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mrand_recs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_random_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-285-d3e75ec12bab>\u001b[0m in \u001b[0;36mmake_recommendations_cluster_scored\u001b[1;34m(cluster, likes, cutoff, data, k, remove_likes, score)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_recommendations_cluster_scored\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_likes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"distance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mliked_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretrieve_liked_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlike_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlikes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT msd_id FROM tracks WHERE %s_cluster = '%d' ORDER BY msd_id\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-55f7bf4d985c>\u001b[0m in \u001b[0;36mretrieve_liked_clusters\u001b[1;34m(cluster, likes)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmsd_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlikes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmsd_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mliked_clusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mliked_clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_scores_mod = {}\n",
    "for key in 'ABCDEFGHIJ':\n",
    "    print key\n",
    "    all_scores_mod[key] = get_scores(key, splitted_likes, [1,5,20,40])\n",
    "    display_scores(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='width:100%'><tr><th>Feature</br> Vector</th><th>Clusters used</th><th>Our Approach</th><th>CF Recommender</th><th>Random Recommender</th><th>Recommendation Size (% of total songs)</th></tr><tr><td rowspan='4'>A</td><td>1</td><td>0.87% (&#177; 4.37)</td><td>0.84% (&#177; 4.34)</td><td>0.47% (&#177; 3.62)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.86% (&#177; 9.36)</td><td>3.21% (&#177; 8.88)</td><td>3.28% (&#177; 8.93)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>13.89% (&#177; 17.77)</td><td>9.73% (&#177; 13.62)</td><td>12.02% (&#177; 13.96)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>27.49% (&#177; 21.36)</td><td>18.26% (&#177; 19.34)</td><td>25.40% (&#177; 20.99)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>C</td><td>1</td><td>0.62% (&#177; 4.19)</td><td>0.84% (&#177; 4.34)</td><td>0.58% (&#177; 3.40)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.72% (&#177; 9.05)</td><td>3.21% (&#177; 8.88)</td><td>2.90% (&#177; 7.79)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>13.12% (&#177; 16.16)</td><td>9.73% (&#177; 13.62)</td><td>12.94% (&#177; 16.34)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>27.27% (&#177; 21.22)</td><td>18.26% (&#177; 19.34)</td><td>27.75% (&#177; 22.25)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>B</td><td>1</td><td>0.78% (&#177; 4.23)</td><td>0.84% (&#177; 4.34)</td><td>0.28% (&#177; 1.84)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.68% (&#177; 8.58)</td><td>3.21% (&#177; 8.88)</td><td>3.58% (&#177; 8.83)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>14.49% (&#177; 18.15)</td><td>9.73% (&#177; 13.62)</td><td>13.19% (&#177; 15.58)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>26.62% (&#177; 21.08)</td><td>18.26% (&#177; 19.34)</td><td>26.82% (&#177; 21.99)</td><td>24.98% (&#177; 0.15)</td></tr><tr><td rowspan='4'>E</td><td>1</td><td>1.12% (&#177; 4.89)</td><td>0.84% (&#177; 4.34)</td><td>0.60% (&#177; 3.11)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.80% (&#177; 9.17)</td><td>3.21% (&#177; 8.88)</td><td>2.93% (&#177; 9.00)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>14.36% (&#177; 16.25)</td><td>9.73% (&#177; 13.62)</td><td>12.02% (&#177; 14.59)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>27.45% (&#177; 21.77)</td><td>18.26% (&#177; 19.34)</td><td>26.85% (&#177; 22.45)</td><td>24.99% (&#177; 0.04)</td></tr><tr><td rowspan='4'>D</td><td>1</td><td>1.20% (&#177; 5.82)</td><td>0.84% (&#177; 4.34)</td><td>0.58% (&#177; 4.12)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>4.16% (&#177; 11.04)</td><td>3.21% (&#177; 8.88)</td><td>3.06% (&#177; 7.99)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>17.35% (&#177; 19.28)</td><td>9.73% (&#177; 13.62)</td><td>11.38% (&#177; 15.10)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>30.58% (&#177; 23.28)</td><td>18.26% (&#177; 19.34)</td><td>24.78% (&#177; 21.42)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>G</td><td>1</td><td>0.84% (&#177; 4.24)</td><td>0.84% (&#177; 4.34)</td><td>0.73% (&#177; 4.27)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.75% (&#177; 9.09)</td><td>3.21% (&#177; 8.88)</td><td>3.22% (&#177; 8.95)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>11.64% (&#177; 15.65)</td><td>9.73% (&#177; 13.62)</td><td>13.27% (&#177; 18.05)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>25.29% (&#177; 21.48)</td><td>18.26% (&#177; 19.34)</td><td>24.63% (&#177; 21.09)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>F</td><td>1</td><td>0.74% (&#177; 4.17)</td><td>0.84% (&#177; 4.34)</td><td>0.62% (&#177; 3.53)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.53% (&#177; 8.82)</td><td>3.21% (&#177; 8.88)</td><td>3.25% (&#177; 9.89)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>13.70% (&#177; 16.81)</td><td>9.73% (&#177; 13.62)</td><td>13.17% (&#177; 17.06)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>25.46% (&#177; 21.67)</td><td>18.26% (&#177; 19.34)</td><td>25.73% (&#177; 21.97)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>I</td><td>1</td><td>0.61% (&#177; 3.58)</td><td>0.84% (&#177; 4.34)</td><td>0.62% (&#177; 3.56)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.08% (&#177; 8.79)</td><td>3.21% (&#177; 8.88)</td><td>3.37% (&#177; 9.25)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>13.05% (&#177; 17.11)</td><td>9.73% (&#177; 13.62)</td><td>12.71% (&#177; 16.47)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>25.07% (&#177; 22.29)</td><td>18.26% (&#177; 19.34)</td><td>25.81% (&#177; 21.06)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>H</td><td>1</td><td>0.86% (&#177; 4.73)</td><td>0.84% (&#177; 4.34)</td><td>1.52% (&#177; 6.75)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.61% (&#177; 8.82)</td><td>3.21% (&#177; 8.88)</td><td>2.89% (&#177; 8.17)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>15.23% (&#177; 18.47)</td><td>9.73% (&#177; 13.62)</td><td>11.89% (&#177; 15.13)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>29.74% (&#177; 22.68)</td><td>18.26% (&#177; 19.34)</td><td>24.77% (&#177; 21.52)</td><td>24.99% (&#177; 0.00)</td></tr><tr><td rowspan='4'>J</td><td>1</td><td>0.71% (&#177; 4.59)</td><td>0.84% (&#177; 4.34)</td><td>0.57% (&#177; 4.21)</td><td>0.62% (&#177; 0.00)</td></tr><tr><td>5</td><td>3.82% (&#177; 9.48)</td><td>3.21% (&#177; 8.88)</td><td>2.92% (&#177; 8.41)</td><td>3.12% (&#177; 0.00)</td></tr><tr><td>20</td><td>13.38% (&#177; 16.97)</td><td>9.73% (&#177; 13.62)</td><td>12.39% (&#177; 15.03)</td><td>12.50% (&#177; 0.00)</td></tr><tr><td>40</td><td>26.26% (&#177; 23.12)</td><td>18.26% (&#177; 19.34)</td><td>25.67% (&#177; 21.79)</td><td>24.99% (&#177; 0.00)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_scores(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'TRSTVVF128F42BA445', u'TRDREXV128F146265C', u'TRTSIXM12903CABAA0']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_recommendations_cluster_scored('K', ['TRAGYLG128F14928B5'],3,data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scored_scores(like_dict, rec_lengths=[1800,1000,500,100,20,5]):\n",
    "    scores = {}\n",
    "    for l in rec_lengths:\n",
    "        scores[l]=[]\n",
    "    for i,user in enumerate(like_dict.keys()):\n",
    "            print i\n",
    "            recs = make_recommendations_cluster_scored('H', like_dict[user]['rec'],1800,data, k=40)\n",
    "            rand_recs = make_random_recommendations(len(recs))\n",
    "            cf_recs = make_cf_recommendations(like_dict[user]['rec'], len(recs))\n",
    "            \n",
    "            for l in rec_lengths:\n",
    "                tp = 0\n",
    "                tp_cf = 0\n",
    "                tp_r = 0\n",
    "                percentage = len(recs)*100./8003.\n",
    "                for track in like_dict[user]['eval']:\n",
    "                    if track in recs[:l]:\n",
    "                        tp += 1\n",
    "                    if track in rand_recs[:l]:\n",
    "                        tp_r += 1\n",
    "                    if track in cf_recs[:l]:\n",
    "                        tp_cf += 1\n",
    "                score = tp*100./len(like_dict[user]['eval'])\n",
    "                score_cf = tp_cf*100./len(like_dict[user]['eval'])            \n",
    "                score_random = tp_r*100./len(like_dict[user]['eval'])\n",
    "                scores[l].append([score, score_cf, score_random, percentage])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "PROGRESS: Getting similar items completed in 0.003273\n",
      "1\n",
      "PROGRESS: Getting similar items completed in 0.002488\n",
      "2\n",
      "PROGRESS: Getting similar items completed in 0.003443\n",
      "3\n",
      "PROGRESS: Getting similar items completed in 0.00215\n",
      "4\n",
      "PROGRESS: Getting similar items completed in 0.002535\n",
      "5\n",
      "PROGRESS: Getting similar items completed in 0.003094\n",
      "6\n",
      "PROGRESS: Getting similar items completed in 0.002359\n",
      "7\n",
      "PROGRESS: Getting similar items completed in 0.002144\n",
      "8\n",
      "PROGRESS: Getting similar items completed in 0.001965\n",
      "9\n",
      "PROGRESS: Getting similar items completed in 0.002113\n",
      "10\n",
      "PROGRESS: Getting similar items completed in 0.00252\n",
      "11\n",
      "PROGRESS: Getting similar items completed in 0.002303\n",
      "12\n",
      "PROGRESS: Getting similar items completed in 0.003721\n",
      "13\n",
      "PROGRESS: Getting similar items completed in 0.002104\n",
      "14\n",
      "PROGRESS: Getting similar items completed in 0.001902\n",
      "15\n",
      "PROGRESS: Getting similar items completed in 0.001695\n",
      "16\n",
      "PROGRESS: Getting similar items completed in 0.00187\n",
      "17\n",
      "PROGRESS: Getting similar items completed in 0.002056\n",
      "18\n",
      "PROGRESS: Getting similar items completed in 0.001972\n",
      "19\n",
      "PROGRESS: Getting similar items completed in 0.001787\n",
      "20\n",
      "PROGRESS: Getting similar items completed in 0.001844\n",
      "21\n",
      "PROGRESS: Getting similar items completed in 0.001813\n",
      "22\n",
      "PROGRESS: Getting similar items completed in 0.002096\n",
      "23\n",
      "PROGRESS: Getting similar items completed in 0.002112\n",
      "24\n",
      "PROGRESS: Getting similar items completed in 0.002113\n",
      "25\n",
      "PROGRESS: Getting similar items completed in 0.001992\n",
      "26\n",
      "PROGRESS: Getting similar items completed in 0.00198\n",
      "27\n",
      "PROGRESS: Getting similar items completed in 0.002145\n",
      "28\n",
      "PROGRESS: Getting similar items completed in 0.003578\n",
      "29\n",
      "PROGRESS: Getting similar items completed in 0.001816\n",
      "30\n",
      "PROGRESS: Getting similar items completed in 0.001923\n",
      "31\n",
      "PROGRESS: Getting similar items completed in 0.003107\n",
      "32\n",
      "PROGRESS: Getting similar items completed in 0.002093\n",
      "33\n",
      "PROGRESS: Getting similar items completed in 0.003048\n",
      "34\n",
      "PROGRESS: Getting similar items completed in 0.002792\n",
      "35\n",
      "PROGRESS: Getting similar items completed in 0.003403\n",
      "36\n",
      "PROGRESS: Getting similar items completed in 0.001882\n",
      "37\n",
      "PROGRESS: Getting similar items completed in 0.002246\n",
      "38\n",
      "PROGRESS: Getting similar items completed in 0.002079\n",
      "39\n",
      "PROGRESS: Getting similar items completed in 0.002267\n",
      "40\n",
      "PROGRESS: Getting similar items completed in 0.002104\n",
      "41\n",
      "PROGRESS: Getting similar items completed in 0.002253\n",
      "42\n",
      "PROGRESS: Getting similar items completed in 0.001978\n",
      "43\n",
      "PROGRESS: Getting similar items completed in 0.002224\n",
      "44\n",
      "PROGRESS: Getting similar items completed in 0.002218\n",
      "45\n",
      "PROGRESS: Getting similar items completed in 0.002388\n",
      "46\n",
      "PROGRESS: Getting similar items completed in 0.001727\n",
      "47\n",
      "PROGRESS: Getting similar items completed in 0.001739\n",
      "48\n",
      "PROGRESS: Getting similar items completed in 0.00198\n",
      "49\n",
      "PROGRESS: Getting similar items completed in 0.001852\n",
      "50\n",
      "PROGRESS: Getting similar items completed in 0.001791\n",
      "51\n",
      "PROGRESS: Getting similar items completed in 0.00581\n",
      "52\n",
      "PROGRESS: Getting similar items completed in 0.00187\n",
      "53\n",
      "PROGRESS: Getting similar items completed in 0.001869\n",
      "54\n",
      "PROGRESS: Getting similar items completed in 0.00162\n",
      "55\n",
      "PROGRESS: Getting similar items completed in 0.001895\n",
      "56\n",
      "PROGRESS: Getting similar items completed in 0.002029\n",
      "57\n",
      "PROGRESS: Getting similar items completed in 0.001653\n",
      "58\n",
      "PROGRESS: Getting similar items completed in 0.001913\n",
      "59\n",
      "PROGRESS: Getting similar items completed in 0.002123\n",
      "60\n",
      "PROGRESS: Getting similar items completed in 0.002569\n",
      "61\n",
      "PROGRESS: Getting similar items completed in 0.001888\n",
      "62\n",
      "PROGRESS: Getting similar items completed in 0.001991\n",
      "63\n",
      "PROGRESS: Getting similar items completed in 0.002355\n",
      "64\n",
      "PROGRESS: Getting similar items completed in 0.002272\n",
      "65\n",
      "PROGRESS: Getting similar items completed in 0.001825\n",
      "66\n",
      "PROGRESS: Getting similar items completed in 0.001975\n",
      "67\n",
      "PROGRESS: Getting similar items completed in 0.001767\n",
      "68\n",
      "PROGRESS: Getting similar items completed in 0.001992\n",
      "69\n",
      "PROGRESS: Getting similar items completed in 0.001882\n",
      "70\n",
      "PROGRESS: Getting similar items completed in 0.002044\n",
      "71\n",
      "PROGRESS: Getting similar items completed in 0.004686\n",
      "72\n",
      "PROGRESS: Getting similar items completed in 0.00318\n",
      "73\n",
      "PROGRESS: Getting similar items completed in 0.002203\n",
      "74\n",
      "PROGRESS: Getting similar items completed in 0.002333\n",
      "75\n",
      "PROGRESS: Getting similar items completed in 0.002798\n",
      "76\n",
      "PROGRESS: Getting similar items completed in 0.002152\n",
      "77\n",
      "PROGRESS: Getting similar items completed in 0.002322\n",
      "78\n",
      "PROGRESS: Getting similar items completed in 0.001907\n",
      "79\n",
      "PROGRESS: Getting similar items completed in 0.001755\n",
      "80\n",
      "PROGRESS: Getting similar items completed in 0.001863\n",
      "81\n",
      "PROGRESS: Getting similar items completed in 0.001896\n",
      "82\n",
      "PROGRESS: Getting similar items completed in 0.001821\n",
      "83\n",
      "PROGRESS: Getting similar items completed in 0.001731\n",
      "84\n",
      "PROGRESS: Getting similar items completed in 0.001854\n",
      "85\n",
      "PROGRESS: Getting similar items completed in 0.001855\n",
      "86\n",
      "PROGRESS: Getting similar items completed in 0.002358\n",
      "87\n",
      "PROGRESS: Getting similar items completed in 0.002544\n",
      "88\n",
      "PROGRESS: Getting similar items completed in 0.001906\n",
      "89\n",
      "PROGRESS: Getting similar items completed in 0.002181\n",
      "90\n",
      "PROGRESS: Getting similar items completed in 0.002037\n",
      "91\n",
      "PROGRESS: Getting similar items completed in 0.002116\n",
      "92\n",
      "PROGRESS: Getting similar items completed in 0.002706\n",
      "93\n",
      "PROGRESS: Getting similar items completed in 0.002491\n",
      "94\n",
      "PROGRESS: Getting similar items completed in 0.001941\n",
      "95\n",
      "PROGRESS: Getting similar items completed in 0.002029\n",
      "96\n",
      "PROGRESS: Getting similar items completed in 0.001681\n",
      "97\n",
      "PROGRESS: Getting similar items completed in 0.001977\n",
      "98\n",
      "PROGRESS: Getting similar items completed in 0.00299\n",
      "99\n",
      "PROGRESS: Getting similar items completed in 0.001909\n",
      "100\n",
      "PROGRESS: Getting similar items completed in 0.002502\n",
      "101\n",
      "PROGRESS: Getting similar items completed in 0.002529\n",
      "102\n",
      "PROGRESS: Getting similar items completed in 0.002206\n",
      "103\n",
      "PROGRESS: Getting similar items completed in 0.002507\n",
      "104\n",
      "PROGRESS: Getting similar items completed in 0.002321\n",
      "105\n",
      "PROGRESS: Getting similar items completed in 0.001837\n",
      "106\n",
      "PROGRESS: Getting similar items completed in 0.001776\n",
      "107\n",
      "PROGRESS: Getting similar items completed in 0.001792\n",
      "108\n",
      "PROGRESS: Getting similar items completed in 0.001839\n",
      "109\n",
      "PROGRESS: Getting similar items completed in 0.002146\n",
      "110\n",
      "PROGRESS: Getting similar items completed in 0.001807\n",
      "111\n",
      "PROGRESS: Getting similar items completed in 0.002787\n",
      "112\n",
      "PROGRESS: Getting similar items completed in 0.001916\n",
      "113\n",
      "PROGRESS: Getting similar items completed in 0.001864\n",
      "114\n",
      "PROGRESS: Getting similar items completed in 0.002355\n",
      "115\n",
      "PROGRESS: Getting similar items completed in 0.00191\n",
      "116\n",
      "PROGRESS: Getting similar items completed in 0.00267\n",
      "117\n",
      "PROGRESS: Getting similar items completed in 0.00184\n",
      "118\n",
      "PROGRESS: Getting similar items completed in 0.0022\n",
      "119\n",
      "PROGRESS: Getting similar items completed in 0.001736\n",
      "120\n",
      "PROGRESS: Getting similar items completed in 0.002726\n",
      "121\n",
      "PROGRESS: Getting similar items completed in 0.002444\n",
      "122\n",
      "PROGRESS: Getting similar items completed in 0.001899\n",
      "123\n",
      "PROGRESS: Getting similar items completed in 0.002076\n",
      "124\n",
      "PROGRESS: Getting similar items completed in 0.001957\n",
      "125\n",
      "PROGRESS: Getting similar items completed in 0.002061\n",
      "126\n",
      "PROGRESS: Getting similar items completed in 0.002989\n",
      "127\n",
      "PROGRESS: Getting similar items completed in 0.001819\n",
      "128\n",
      "PROGRESS: Getting similar items completed in 0.003354\n",
      "129\n",
      "PROGRESS: Getting similar items completed in 0.001938\n",
      "130\n",
      "PROGRESS: Getting similar items completed in 0.002069\n",
      "131\n",
      "PROGRESS: Getting similar items completed in 0.002054\n",
      "132\n",
      "PROGRESS: Getting similar items completed in 0.00199\n",
      "133\n",
      "PROGRESS: Getting similar items completed in 0.00243\n",
      "134\n",
      "PROGRESS: Getting similar items completed in 0.002225\n",
      "135\n",
      "PROGRESS: Getting similar items completed in 0.001965\n",
      "136\n",
      "PROGRESS: Getting similar items completed in 0.002105\n",
      "137\n",
      "PROGRESS: Getting similar items completed in 0.002351\n",
      "138\n",
      "PROGRESS: Getting similar items completed in 0.002408\n",
      "139\n",
      "PROGRESS: Getting similar items completed in 0.00233\n",
      "140\n",
      "PROGRESS: Getting similar items completed in 0.001775\n",
      "141\n",
      "PROGRESS: Getting similar items completed in 0.002483\n",
      "142\n",
      "PROGRESS: Getting similar items completed in 0.002643\n",
      "143\n",
      "PROGRESS: Getting similar items completed in 0.001793\n",
      "144\n",
      "PROGRESS: Getting similar items completed in 0.001737\n",
      "145\n",
      "PROGRESS: Getting similar items completed in 0.001836\n",
      "146\n",
      "PROGRESS: Getting similar items completed in 0.001726\n",
      "147\n",
      "PROGRESS: Getting similar items completed in 0.002122\n",
      "148\n",
      "PROGRESS: Getting similar items completed in 0.001694\n",
      "149\n",
      "PROGRESS: Getting similar items completed in 0.0024\n",
      "150\n",
      "PROGRESS: Getting similar items completed in 0.002392\n",
      "151\n",
      "PROGRESS: Getting similar items completed in 0.001871\n",
      "152\n",
      "PROGRESS: Getting similar items completed in 0.001916\n",
      "153\n",
      "PROGRESS: Getting similar items completed in 0.002363\n",
      "154\n",
      "PROGRESS: Getting similar items completed in 0.001809\n",
      "155\n",
      "PROGRESS: Getting similar items completed in 0.001706\n",
      "156\n",
      "PROGRESS: Getting similar items completed in 0.002341\n",
      "157\n",
      "PROGRESS: Getting similar items completed in 0.001584\n",
      "158\n",
      "PROGRESS: Getting similar items completed in 0.002564\n",
      "159\n",
      "PROGRESS: Getting similar items completed in 0.001796\n",
      "160\n",
      "PROGRESS: Getting similar items completed in 0.001809\n",
      "161\n",
      "PROGRESS: Getting similar items completed in 0.002298\n",
      "162\n",
      "PROGRESS: Getting similar items completed in 0.001991\n",
      "163\n",
      "PROGRESS: Getting similar items completed in 0.001888\n",
      "164\n",
      "PROGRESS: Getting similar items completed in 0.001912\n",
      "165\n",
      "PROGRESS: Getting similar items completed in 0.001957\n",
      "166\n",
      "PROGRESS: Getting similar items completed in 0.002157\n",
      "167\n",
      "PROGRESS: Getting similar items completed in 0.001872\n",
      "168\n",
      "PROGRESS: Getting similar items completed in 0.002526\n",
      "169\n",
      "PROGRESS: Getting similar items completed in 0.001762\n",
      "170\n",
      "PROGRESS: Getting similar items completed in 0.001859\n",
      "171\n",
      "PROGRESS: Getting similar items completed in 0.002084\n",
      "172\n",
      "PROGRESS: Getting similar items completed in 0.002104\n",
      "173\n",
      "PROGRESS: Getting similar items completed in 0.002071\n",
      "174\n",
      "PROGRESS: Getting similar items completed in 0.00259\n",
      "175\n",
      "PROGRESS: Getting similar items completed in 0.002668\n",
      "176\n",
      "PROGRESS: Getting similar items completed in 0.001884\n",
      "177\n",
      "PROGRESS: Getting similar items completed in 0.001672\n",
      "178\n",
      "PROGRESS: Getting similar items completed in 0.00247\n",
      "179\n",
      "PROGRESS: Getting similar items completed in 0.001868\n",
      "180\n",
      "PROGRESS: Getting similar items completed in 0.002113\n",
      "181\n",
      "PROGRESS: Getting similar items completed in 0.003061\n",
      "182\n",
      "PROGRESS: Getting similar items completed in 0.001885\n",
      "183\n",
      "PROGRESS: Getting similar items completed in 0.00189\n",
      "184\n",
      "PROGRESS: Getting similar items completed in 0.002032\n",
      "185\n",
      "PROGRESS: Getting similar items completed in 0.001987\n",
      "186\n",
      "PROGRESS: Getting similar items completed in 0.002994\n",
      "187\n",
      "PROGRESS: Getting similar items completed in 0.001842\n",
      "188\n",
      "PROGRESS: Getting similar items completed in 0.001736\n",
      "189\n",
      "PROGRESS: Getting similar items completed in 0.002343\n",
      "190\n",
      "PROGRESS: Getting similar items completed in 0.001904\n",
      "191\n",
      "PROGRESS: Getting similar items completed in 0.001749\n",
      "192\n",
      "PROGRESS: Getting similar items completed in 0.002917\n",
      "193\n",
      "PROGRESS: Getting similar items completed in 0.001976\n",
      "194\n",
      "PROGRESS: Getting similar items completed in 0.002112\n",
      "195\n",
      "PROGRESS: Getting similar items completed in 0.002096\n",
      "196\n",
      "PROGRESS: Getting similar items completed in 0.002309\n",
      "197\n",
      "PROGRESS: Getting similar items completed in 0.001757\n",
      "198\n",
      "PROGRESS: Getting similar items completed in 0.002854\n",
      "199\n",
      "PROGRESS: Getting similar items completed in 0.001804\n",
      "200\n",
      "PROGRESS: Getting similar items completed in 0.002069\n",
      "201\n",
      "PROGRESS: Getting similar items completed in 0.002446\n",
      "202\n",
      "PROGRESS: Getting similar items completed in 0.003556\n",
      "203\n",
      "PROGRESS: Getting similar items completed in 0.002556\n",
      "204\n",
      "PROGRESS: Getting similar items completed in 0.002768\n",
      "205\n",
      "PROGRESS: Getting similar items completed in 0.002141\n",
      "206\n",
      "PROGRESS: Getting similar items completed in 0.001844\n",
      "207\n",
      "PROGRESS: Getting similar items completed in 0.001995\n",
      "208\n",
      "PROGRESS: Getting similar items completed in 0.002601\n",
      "209\n",
      "PROGRESS: Getting similar items completed in 0.002087\n",
      "210\n",
      "PROGRESS: Getting similar items completed in 0.002711\n",
      "211\n",
      "PROGRESS: Getting similar items completed in 0.002603\n",
      "212\n",
      "PROGRESS: Getting similar items completed in 0.002604\n",
      "213\n",
      "PROGRESS: Getting similar items completed in 0.00574\n",
      "214\n",
      "PROGRESS: Getting similar items completed in 0.001988\n",
      "215\n",
      "PROGRESS: Getting similar items completed in 0.002145\n",
      "216\n",
      "PROGRESS: Getting similar items completed in 0.001999\n",
      "217\n",
      "PROGRESS: Getting similar items completed in 0.001992\n",
      "218\n",
      "PROGRESS: Getting similar items completed in 0.002632\n",
      "219\n",
      "PROGRESS: Getting similar items completed in 0.001942\n",
      "220\n",
      "PROGRESS: Getting similar items completed in 0.002053\n",
      "221\n",
      "PROGRESS: Getting similar items completed in 0.002647\n",
      "222\n",
      "PROGRESS: Getting similar items completed in 0.001829\n",
      "223\n",
      "PROGRESS: Getting similar items completed in 0.002963\n",
      "224\n",
      "PROGRESS: Getting similar items completed in 0.003141\n",
      "225\n",
      "PROGRESS: Getting similar items completed in 0.002454\n",
      "226\n",
      "PROGRESS: Getting similar items completed in 0.001937\n",
      "227\n",
      "PROGRESS: Getting similar items completed in 0.001975\n",
      "228\n",
      "PROGRESS: Getting similar items completed in 0.002986\n",
      "229\n",
      "PROGRESS: Getting similar items completed in 0.001791\n",
      "230\n",
      "PROGRESS: Getting similar items completed in 0.002127\n",
      "231\n",
      "PROGRESS: Getting similar items completed in 0.002691\n",
      "232\n",
      "PROGRESS: Getting similar items completed in 0.002866\n",
      "233\n",
      "PROGRESS: Getting similar items completed in 0.002017\n",
      "234\n",
      "PROGRESS: Getting similar items completed in 0.001873\n",
      "235\n",
      "PROGRESS: Getting similar items completed in 0.001867\n",
      "236\n",
      "PROGRESS: Getting similar items completed in 0.002675\n",
      "237\n",
      "PROGRESS: Getting similar items completed in 0.002715\n",
      "238\n",
      "PROGRESS: Getting similar items completed in 0.001971\n",
      "239\n",
      "PROGRESS: Getting similar items completed in 0.001975\n",
      "240\n",
      "PROGRESS: Getting similar items completed in 0.006673\n",
      "241\n",
      "PROGRESS: Getting similar items completed in 0.001836\n",
      "242\n",
      "PROGRESS: Getting similar items completed in 0.001939\n",
      "243\n",
      "PROGRESS: Getting similar items completed in 0.00255\n",
      "244\n",
      "PROGRESS: Getting similar items completed in 0.002243\n",
      "245\n",
      "PROGRESS: Getting similar items completed in 0.001972\n",
      "246\n",
      "PROGRESS: Getting similar items completed in 0.002072\n",
      "247\n",
      "PROGRESS: Getting similar items completed in 0.002073\n",
      "248\n",
      "PROGRESS: Getting similar items completed in 0.002418\n",
      "249\n",
      "PROGRESS: Getting similar items completed in 0.001928\n"
     ]
    }
   ],
   "source": [
    "scored_scores = get_scored_scores(splitted_likes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$100$ & $1.23\\% (\\pm 4.62) $&$ 1.05\\% (\\pm 4.72) $&$ 1.71\\% (\\pm 6.44) $ \\\\ \\hline \n",
      "\n",
      "$5$ & $0.00\\% (\\pm 0.00) $&$ 0.16\\% (\\pm 2.14) $&$ 0.00\\% (\\pm 0.00) $ \\\\ \\hline \n",
      "\n",
      "$1800$ & $26.59\\% (\\pm 23.35) $&$ 2.92\\% (\\pm 7.02) $&$ 20.56\\% (\\pm 17.86) $ \\\\ \\hline \n",
      "\n",
      "$1000$ & $14.90\\% (\\pm 18.34) $&$ 2.92\\% (\\pm 7.02) $&$ 11.00\\% (\\pm 13.90) $ \\\\ \\hline \n",
      "\n",
      "$500$ & $7.37\\% (\\pm 14.16) $&$ 2.92\\% (\\pm 7.02) $&$ 5.63\\% (\\pm 10.15) $ \\\\ \\hline \n",
      "\n",
      "$20$ & $0.10\\% (\\pm 1.58) $&$ 0.36\\% (\\pm 2.75) $&$ 0.24\\% (\\pm 1.88) $ \\\\ \\hline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for length,user_scores in scored_scores.iteritems():\n",
    "    mean =  np.mean(user_scores, axis=0)\n",
    "    std = np.std(user_scores, axis=0)\n",
    "    print \"$%d$ & $%.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $&$ %.2f\\\\%% (\\\\pm %.2f) $ \\\\\\\\ \\\\hline \\n\" % (length,mean[0], std[0], mean[1], std[1], mean[2], std[2],) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_outperforms(like_dict, feature, rec_lengths=[50,250,1000,2000]):\n",
    "    A = {}\n",
    "    B = {}\n",
    "    f = FloatProgress(min=0, max=100)\n",
    "    display(f)\n",
    "    f.description = feature\n",
    "    for l in rec_lengths:\n",
    "        A[l] = 0\n",
    "        B[l] = 0\n",
    "    for i,user in enumerate(like_dict.keys()):\n",
    "            f.value = float(i)*100/len(like_dict)\n",
    "            #recs = make_recommendations_cluster_scored('H', like_dict[user]['rec'],1800, data)\n",
    "            recs = make_recommendations_cluster_scored(feature, like_dict[user]['rec'],2000,data[feature]['data'].T,k=40)\n",
    "            #cf_recs = make_cf_recommendations(like_dict[user]['rec'], 1000)\n",
    "            recs_b = make_random_recommendations(len(recs))\n",
    "            \n",
    "            for l in rec_lengths:\n",
    "                tp = 0\n",
    "                tp_b = 0\n",
    "                for track in like_dict[user]['eval']:\n",
    "                    if track in recs[:l]:\n",
    "                        tp += 1\n",
    "                    if track in recs_b[:l]:\n",
    "                        tp_b += 1\n",
    "                if(tp > tp_b):\n",
    "                    A[l] += 1\n",
    "                elif(tp < tp_b):\n",
    "                    B[l] += 1\n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signsH = {}\n",
    "for i in range(50):\n",
    "    A,B = count_outperforms(splitted_likes, 'H')\n",
    "    for i in reversed([2000,1000,250,50]):\n",
    "            if signsH.has_key(i):\n",
    "                signsH[i].append(sign_test(A[i],B[i]))\n",
    "            else:\n",
    "                signsH[i] = [sign_test(A[i],B[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A,B = count_outperforms(splitted_likes, 'H') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50: 17, 250: 56, 1000: 88, 2000: 102}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50: 11, 250: 28, 1000: 66, 2000: 76}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import factorial as fac\n",
    "def sign_test(A,B):\n",
    "    p = 0.\n",
    "    n = (A+B)\n",
    "    for i in range(A,n):\n",
    "        p += fac(n)/ (fac(i)*fac(n-i))\n",
    "    p *= (0.5)**(n)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signs = {}\n",
    "for key in \"ABCDEFGHIJ\":\n",
    "    result={}\n",
    "    for j in range(10):\n",
    "        A,B = count_outperforms(splitted_likes, key) \n",
    "        for i in reversed([2000,1000,250,50]):\n",
    "            if result.has_key(i):\n",
    "                result[i].append(sign_test(A[i],B[i]))\n",
    "            else:\n",
    "                result[i] = [sign_test(A[i],B[i])]\n",
    "    for i in reversed([2000,1000,250,50]):\n",
    "        result[i] = np.mean(result[i])\n",
    "    signs[key] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = \"N & A & B & C & D & E & F & G & H & I & J & \\\\\\\\ \\\\hline \\n\"\n",
    "for i in reversed([2000,1000,250,50]):\n",
    "    output += \"$%d$\\t&\\t\" % i\n",
    "    for key in \"ABCDEFGHIJ\":\n",
    "        output += \"$%.2f$\\t&\\t\" % signs[key][i]\n",
    "    output += \"\\\\\\\\ \\\\hline \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N & A & B & C & D & E & F & G & H & I & J & \\\\ \\hline \n",
      "$50$\t&\t$0.83$\t&\t$0.32$\t&\t$0.71$\t&\t$0.64$\t&\t$0.25$\t&\t$0.47$\t&\t$0.57$\t&\t$0.07$\t&\t$0.75$\t&\t$0.45$\t&\t\\\\ \\hline \n",
      "$250$\t&\t$0.34$\t&\t$0.44$\t&\t$0.57$\t&\t$0.30$\t&\t$0.15$\t&\t$0.54$\t&\t$0.51$\t&\t$0.02$\t&\t$0.38$\t&\t$0.31$\t&\t\\\\ \\hline \n",
      "$1000$\t&\t$0.41$\t&\t$0.18$\t&\t$0.23$\t&\t$0.09$\t&\t$0.08$\t&\t$0.41$\t&\t$0.59$\t&\t$0.08$\t&\t$0.27$\t&\t$0.13$\t&\t\\\\ \\hline \n",
      "$2000$\t&\t$0.06$\t&\t$0.17$\t&\t$0.22$\t&\t$0.11$\t&\t$0.20$\t&\t$0.47$\t&\t$0.49$\t&\t$0.09$\t&\t$0.33$\t&\t$0.04$\t&\t\\\\ \\hline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usage_prediction(like_dict, feature, rec_lengths=[2000,1000,250,50]):\n",
    "    user_scores = {}\n",
    "    f = FloatProgress(min=0, max=100)\n",
    "    display(f)\n",
    "    for l in rec_lengths:\n",
    "        user_scores[l] = []\n",
    "    for i,user in enumerate(like_dict.keys()):\n",
    "            f.value = float(i)*100/len(like_dict)\n",
    "            for l in rec_lengths:\n",
    "                recs = make_recommendations_cluster_scored(feature, like_dict[user]['rec'],2000,data[feature]['data'].T, k=l/50)\n",
    "                #cf_recs = make_random_recommendations(len(recs)) #make_cf_recommendations(like_dict[user]['rec'], len(recs))\n",
    "                ev = set(like_dict[user]['eval'])\n",
    "                rec_set = set(recs[:l])\n",
    "                #cf_rec_set = set(cf_recs[:l])\n",
    "                \n",
    "                tp = len(rec_set & ev) *1.\n",
    "                fp = len(rec_set - ev) *1.\n",
    "                fn = len(ev) - tp *1.\n",
    "                tn = 8003. - (len(rec_set)+fn) * 1.\n",
    "\n",
    "                #precision, recall, false positive rate\n",
    "                user_scores[l].append([tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)])\n",
    "    scores = {}\n",
    "    for key, values in user_scores.iteritems():\n",
    "        mean = np.mean(values, axis=0)\n",
    "        std = np.std(values, axis=0)\n",
    "        scores[key] = np.asarray(zip(mean,std)).flatten()\n",
    "    return user_scores,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_scores, scores = usage_prediction(splitted_likes,'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$50$ & $0.104\\% (\\pm 0.479\\%)$& $0.874\\% (\\pm 4.371\\%)$&$ 0.625\\% (\\pm 0.003\\%)$ \\\\ \\hline\n",
      "$250$ & $0.085\\% (\\pm 0.178\\%)$& $3.865\\% (\\pm 9.363\\%)$&$ 3.123\\% (\\pm 0.005\\%)$ \\\\ \\hline\n",
      "$1000$ & $0.082\\% (\\pm 0.105\\%)$& $13.888\\% (\\pm 17.772\\%)$&$ 12.494\\% (\\pm 0.011\\%)$ \\\\ \\hline\n",
      "$2000$ & $0.082\\% (\\pm 0.078\\%)$& $27.492\\% (\\pm 21.363\\%)$&$ 24.988\\% (\\pm 0.013\\%)$ \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(scores.keys()):\n",
    "    print \"$\" + str(key) + \"$ & $%2.3f\\\\%% (\\\\pm %2.3f\\\\%%)$& $%2.3f\\\\%% (\\\\pm %2.3f\\\\%%)$&$ %2.3f\\\\%% (\\\\pm %2.3f\\\\%%)$ \\\\\\\\ \\hline\" % tuple(scores[key][:6]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROC_B = []\n",
    "for key in sorted(scores.keys()):\n",
    "    ROC_B.append(scores[key][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rscore(recs, ev, alpha):\n",
    "    score = 0\n",
    "    for i,r in enumerate(recs):\n",
    "        if r in ev:\n",
    "            score += 1 / 2**(i/alpha - 1)\n",
    "            #print \"hit at %d\" % i\n",
    "    max_score = 0\n",
    "    for i,r in enumerate(ev):\n",
    "        max_score += 1 / 2**(i/alpha - 1)\n",
    "    return score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rscore(like_dict,alpha):\n",
    "    rscores = []\n",
    "    #rscores_b = []\n",
    "    max_rscore= []\n",
    "    f = FloatProgress(min=0, max=100)\n",
    "    display(f)\n",
    "    for i,user in enumerate(like_dict.keys()):\n",
    "        f.value = float(i)*100/len(like_dict)\n",
    "        recs = make_recommendations_cluster_scored('B', like_dict[user]['rec'],100,data['B']['data'].T, k=5)\n",
    "        #recs = make_random_recommendations(100)\n",
    "        rscore_user, max_rscore_user = rscore(recs,like_dict[user]['eval'],alpha)\n",
    "        #print \"our %f %f\" % (rscore_user, max_rscore_user)\n",
    "        #rand_rscore_user, max_rscore_user = rscore(rand_recs,like_dict[user]['eval'],alpha)\n",
    "        #print \"rand %f %f \\n\" % (rand_rscore_user, max_rscore_user)\n",
    "        rscores.append(rscore_user)\n",
    "        #rscores_b.append(rand_rscore_user)\n",
    "        max_rscore.append(max_rscore_user)\n",
    "    r = 100* sum(rscores)/sum(max_rscore)\n",
    "    #r_b = 100* sum(rscores_b)/sum(max_rscore)\n",
    "    return r#,r_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003402817041039896"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rscore(splitted_likes,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.11685673079686356 #H Rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.02536004547366449 #A RScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.085284235452609936"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r) #Random Rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05213995098123266"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.05213995098123266 #D Rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_likes = {}\n",
    "for key,values in user_likes.iteritems():\n",
    "    new_likes[key] = Counter(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "splitted_likes = {}\n",
    "p = 0.9\n",
    "for user in new_likes.keys():\n",
    "    likes = new_likes[user].keys()\n",
    "    first = int(floor(len(likes) * p))\n",
    "    last = len(likes) - first\n",
    "    random.shuffle(likes)\n",
    "    splitted_likes[user] = {'rec':likes[:first], 'eval':likes[-last:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for val in splitted_likes.values():\n",
    "    if len(set(val['rec']) & set(val['eval'])) > 0:\n",
    "        print \"error\"\n",
    "    if len(val['rec']) < 1 or len(val['eval']) < 1:\n",
    "        print 'too small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval': ['TROSAAV128F93424E7',\n",
       "  'TRLWWYP128F92ECD5F',\n",
       "  'TRGQECM128F4230D0C',\n",
       "  'TRMVVVK128F9307B40',\n",
       "  'TRQVUCJ128F92DF5FE'],\n",
       " 'rec': ['TRSBOOT128F930CD73',\n",
       "  'TRAATZB12903CC59D8',\n",
       "  'TRPTHQP128F427261D',\n",
       "  'TRUINQU128F14A66A6',\n",
       "  'TRJPEPR12903CD7301',\n",
       "  'TRDMRMV128F9330FC2',\n",
       "  'TRZWCRX12903C9700E',\n",
       "  'TRPMNBR128F92F9588',\n",
       "  'TRSPVGX128F427AA3D',\n",
       "  'TREAAMX128F423EAEE',\n",
       "  'TRTWRDZ128F1452BF8',\n",
       "  'TRXNRTN12903CC449D',\n",
       "  'TRDXGYW128F92C5A72',\n",
       "  'TROLYAE128F934C593',\n",
       "  'TRBEBDQ128F1469001',\n",
       "  'TRCBMHE128F149077E',\n",
       "  'TRANPKC128F93486B9',\n",
       "  'TRINAJE128F92FB145',\n",
       "  'TRXZMOQ128F429D983',\n",
       "  'TRXDECL128F93139E5',\n",
       "  'TRPUFBX128F93296CB',\n",
       "  'TROROLJ12903CE6C8B',\n",
       "  'TRAGNIQ128F92F45F9',\n",
       "  'TRZIGOU128F425CEC4',\n",
       "  'TRQXODD128F92D0670',\n",
       "  'TRDWTEB128F1464BE3',\n",
       "  'TRUQDBE128F149EC19',\n",
       "  'TRYQRLB128F93362E2',\n",
       "  'TREITPK128F4289C6D',\n",
       "  'TRXVNDR128F145E0FE',\n",
       "  'TRXNPHC128F147365A',\n",
       "  'TRPXIBW128F42BA3C0',\n",
       "  'TRBUYKI128F92DB14A',\n",
       "  'TRKXNDU128F4263E8C',\n",
       "  'TRAPLDX128F931C321',\n",
       "  'TRFLARJ128F930061B']}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_likes['eb248402322adb243ca168a9bf624504']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
